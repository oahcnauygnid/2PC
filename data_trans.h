#pragma once

#include "torch/script.h"
#include <iostream>
#include <string>

/* mnist cnn
name: con1.0.weight
size: [16, 1, 5, 5]
flatten: [400]
name: con1.0.bias
size: [16]
flatten: [16]
name: con2.0.weight
size: [16, 16, 5, 5]
flatten: [6400]
name: con2.0.bias
size: [16]
flatten: [16]
name: fc1.weight
size: [100, 256]
flatten: [25600]
name: fc1.bias
size: [100]
flatten: [10]
name: fc2.weight
size: [10, 100]
flatten: [1000]
name: fc2.bias
size: [10]
flatten: [10]
*/
float con1_0_weight[16][1][5][5];
float con1_0_bias[16];
float con2_0_weight[16][16][5][5];
float con2_0_bias[16];
float fc1_weight[100][256];
float fc1_bias[100];
float fc2_weight[10][100];
float fc2_bias[10];

float con1_0_weight_extend[16][25];
float con2_0_weight_extend[400][64];

float input[28][28] = {{  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.,   0.,  58., 249., 217.,  30.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.,   3., 174., 254., 164.,  30.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0., 124., 254., 254., 109.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0., 145., 254., 145.,   8.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,  19., 219., 187.,   6.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0., 111., 251., 110.,   6.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,  97., 212., 251.,  64.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,  68., 253., 239.,  53.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,  49., 219., 254., 154.,  12.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0., 112., 246., 241.,  91.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,  82., 247., 254., 206.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0., 126., 254., 247., 109.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            4., 249., 254., 107.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   7.,
          191., 254., 254.,  48.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  71.,
          254., 254., 169.,   3.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  31., 241.,
          255., 194.,  31.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  31., 188., 254.,
          254.,  93.,   6.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  10., 186., 254.,
          254., 122.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  67., 254., 253.,
          234.,   6.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  89., 210., 241.,
            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.},
         {  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
            0.,   0.,   0.,   0.,   0.,   0.}
        };

float con1_input_extend[25][496];
float con1_xc_extend[25][496];
float con1_xs_extend[25][496];
float con2_input_extend[400][64];

void get_double_params(std::string model_path) {
         
    torch::jit::Module module = torch::jit::load(model_path.c_str());
    int i = 0;
    for(auto params: module.named_parameters()){
        auto data = params.value.data();
        switch (i)
        {
        case 0:
            for (size_t i0 = 0; i0 < 16; i0++)
                for (size_t i1 = 0; i1 < 1; i1++)
                    for (size_t i2 = 0; i2 < 5; i2++)
                        for (size_t i3 = 0; i3 < 5; i3++)
                            con1_0_weight[i0][i1][i2][i3] = data[i0][i1][i2][i3].item().toFloat();
            break;
        case 1:
            for (size_t i0 = 0; i0 < 16; i0++)
                con1_0_bias[i0] = data[i0].item().toFloat();
            break;
        case 2:
            for (size_t i0 = 0; i0 < 16; i0++)
                for (size_t i1 = 0; i1 < 16; i1++)
                    for (size_t i2 = 0; i2 < 5; i2++)
                        for (size_t i3 = 0; i3 < 5; i3++)
                            con2_0_weight[i0][i1][i2][i3] = data[i0][i1][i2][i3].item().toFloat();
            break;
        case 3:
            for (size_t i0 = 0; i0 < 16; i0++)
                    con2_0_bias[i0] = data[i0].item().toFloat();
            break;
        case 4:
            for (size_t i0 = 0; i0 < 100; i0++)
                for (size_t i1 = 0; i1 < 256; i1++)
                    fc1_weight[i0][i1] = data[i0][i1].item().toFloat();
            break;
        case 5:
            for (size_t i0 = 0; i0 < 100; i0++)
                fc1_bias[i0] = data[i0].item().toFloat();
            break;
        case 6:
            for (size_t i0 = 0; i0 < 10; i0++)
                for (size_t i1 = 0; i1 < 100; i1++)
                    fc1_weight[i0][i1] = data[i0][i1].item().toFloat();
            break;
        case 7:
            for (size_t i0 = 0; i0 < 10; i0++)
                fc1_bias[i0] = data[i0].item().toFloat();
            break;
        default:
            break;
        }
        i++;
    }
}

void get_rand_params() {
    for (size_t i = 0; i < 25; i++){
        for (size_t j = 0; j < 496; j++){
            con1_xc_extend[i][j] = rand()/(RAND_MAX+0.0);
        }
    }
}

void conv1_input_to_mult(float input[][28]) {
    for (size_t j = 0; j < 496; j++){
        size_t start_x = j%24;
        size_t start_y = j/24;
        for (size_t i = 0; i < 25; i++){
            con1_input_extend[i][j] = input[start_x+i%5][start_y+i/5];
        }
    }
}

void conv1_weight_to_mult() {
    for (size_t i = 0; i < 16; i++){
        for (size_t j = 0; j < 25; j++){
            con1_0_weight_extend[i][j] = con1_0_weight[i][0][j%5][j/5];
        }
    }
}

void conv2_to_mult(float** input) {
    
}
// int main() {
//     string model_path = "/home/dingyc/PPML/mywork/2PC/pytorch_test/models/cnn_mnist/mnist_net.pt";
//     get_double_params(model_path);
//     cout<< "get_double_params ok" << endl;
    
// }